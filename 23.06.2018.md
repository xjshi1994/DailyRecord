package template.spark

import org.apache.log4j.{Level, LogManager, Logger}
import org.apache.spark.sql.SparkSession

trait InitSpark {
  val spark: SparkSession = SparkSession.builder()
                            .appName("Spark example")
                            .master("local[*]")
                            .config("option", "some-value")
                            .getOrCreate()

  val sc = spark.sparkContext
  val sqlContext = spark.read.format("jdbc")
    .option("url", "jdbc:mysql://10.132.1.4:4000/mongo?autoReconnect=true&useSSL=false&serverTimezone=GMT")
    .option("driver", "com.mysql.cj.jdbc.Driver")
    .option("dbtable", "creditReport")
    .option("user", "mongo_select")
    .option("password", "evltAp77g3nh2pen").load()

  def reader = spark.read
               .option("header",true)
               .option("inferSchema", true)
               .option("mode", "DROPMALFORMED")

  def readerWithoutHeader = spark.read
                            .option("header",true)
                            .option("inferSchema", true)
                            .option("mode", "DROPMALFORMED")
  private def init = {
    sc.setLogLevel("ERROR")
    Logger.getLogger("org").setLevel(Level.ERROR)
    Logger.getLogger("akka").setLevel(Level.ERROR)
    LogManager.getRootLogger.setLevel(Level.ERROR)
  }
  init
  def close = {
    spark.close()
  }
}


    val db = sqlContext.createOrReplaceTempView("creditReport")
    val rdDF = spark.sql("select * from creditReport").toDF()
    val jsDF = rdDF.select($"requestId".alias("requestId"), $"timestamp".alias("timestamp"),get_json_object($"responseData","$.creditAmount").alias("creditAmount"),
      get_json_object($"responseData", "$.creditRating").alias("creditRating"),
      get_json_object($"responseData", "$.status").alias("status")
    ).where($"status"==="CREDIT_SUCCESS")
    jsDF.show()

**WHERE CLAUSE uses three '='**
4. procdure to upload job to azkaban
* develop on local or zeppelin
* fork etl
* pull request
* write job file. put this under folder : azkaban_projects/test/run_tidb_script.job _template_: test_tidb_script.job/ for everysql write corresponding job file(revise the root of url)
* zip -r file.zip file dir (zip whole folder) and upload to azkaban


